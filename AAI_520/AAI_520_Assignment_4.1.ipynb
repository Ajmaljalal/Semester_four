{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4667db00",
   "metadata": {},
   "source": [
    "# RAG-based Question Answering System with TED Talks\n",
    "\n",
    "In this assignment, we are exploring how retrieval-augmented generation (RAG) improves language model responses by grounding them in real data. We will be using TED Talk transcripts to combine semantic search with a transformer model for generating accurate, context-aware answers.\n",
    "\n",
    "## Objective\n",
    "Building a simple question answering (QA) system using Retrieval-augmented generation (RAG) techniques with LangChain and HuggingFace tools to load a TED Talks dataset, embed and store document chunks using a vector database (FAISS), and query them using a pretrained transformer model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adb206e",
   "metadata": {},
   "source": [
    "## Step 1: Installing Required Dependencies\n",
    "\n",
    "We are installing the necessary packages for building our RAG system including datasets for loading TED talks, LangChain for document processing and chaining, FAISS for vector storage, and transformers for the language model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1839c089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries for RAG system\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9385e2f3",
   "metadata": {},
   "source": [
    "## Step 2: Loading TED Talks Dataset\n",
    "\n",
    "We are loading a manageable subset of English translations from the TED Talks dataset. We are using the gigant/ted_descriptions\" and limiting to the first 1000 samples to manage memory usage effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecd4d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 TED Talk samples\n"
     ]
    }
   ],
   "source": [
    "# Loading TED Talks dataset with working alternative\n",
    "dataset = load_dataset(\"gigant/ted_descriptions\", split=\"train[:1000]\")  # Limit for memory\n",
    "print(f\"Loaded {len(dataset)} TED Talk samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182c96ef",
   "metadata": {},
   "source": [
    "## Step 3: Converting to LangChain Documents\n",
    "\n",
    "We are converting the dataset items to LangChain Document objects with proper metadata including speaker information for better context during retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4802a857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1000 Document objects\n"
     ]
    }
   ],
   "source": [
    "# Converting dataset items to LangChain Document objects\n",
    "documents = []\n",
    "for item in dataset:\n",
    "    if item.get(\"descr\"):  # Using 'descr' field which contains the description\n",
    "        doc = Document(\n",
    "            page_content=item[\"descr\"],\n",
    "            metadata={\n",
    "                \"url\": item.get(\"url\", \"\"),\n",
    "                \"source\": \"TED Talks\"\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "print(f\"Created {len(documents)} Document objects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001a53bc",
   "metadata": {},
   "source": [
    "## Step 4: Splitting Documents into Chunks\n",
    "\n",
    "We are splitting the documents into smaller chunks for better retrieval performance. Using RecursiveCharacterTextSplitter with appropriate chunk size and overlap to maintain context continuity between chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0b35244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1000 documents into 1230 chunks\n"
     ]
    }
   ],
   "source": [
    "# Splitting documents into manageable chunks for better retrieval\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # Each chunk has 500 characters\n",
    "    chunk_overlap=50  # 50 characters overlap for context continuity\n",
    ")\n",
    "\n",
    "# Split the documents into chunks\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(f\"Split {len(documents)} documents into {len(docs)} chunks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f075e1",
   "metadata": {},
   "source": [
    "## Step 5: Creating Embeddings and Vector Database\n",
    "\n",
    "We are generating embeddings using HuggingFace sentence transformers and creating a FAISS vector database for efficient similarity search. This enables semantic retrieval of relevant document chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "11662c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database created successfully\n"
     ]
    }
   ],
   "source": [
    "# Creating embeddings using HuggingFace sentence transformer\n",
    "embeddings_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Creating FAISS vector database from document chunks\n",
    "db = FAISS.from_documents(docs, embeddings_function)\n",
    "print(\"Vector database created successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2e28b6",
   "metadata": {},
   "source": [
    "## Step 6: Setting up Language Model Pipeline\n",
    "\n",
    "We are initializing the HuggingFace transformer model pipeline for text generation. Using google/flan-t5-small model configured for CPU usage to ensure compatibility and efficient processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58523630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language model pipeline created successfully\n"
     ]
    }
   ],
   "source": [
    "# Setting up device for CPU usage\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Creating HuggingFace pipeline for text generation\n",
    "qa_pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-small\",\n",
    "    max_length=256,\n",
    "    device=device,\n",
    "    do_sample=True,\n",
    "    temperature=0.5,  # Adding some creativity\n",
    "    top_p=0.9\n",
    "\n",
    ")\n",
    "\n",
    "# Wrapping pipeline into LangChain-compatible LLM\n",
    "llm = HuggingFacePipeline(pipeline=qa_pipeline)\n",
    "print(\"Language model pipeline created successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eca75a",
   "metadata": {},
   "source": [
    "## Step 7: Building the RAG Question-Answering Chain\n",
    "\n",
    "We are creating the complete RetrievalQA chain that connects our vector database retriever with the language model. This enables the system to retrieve relevant TED talk chunks and generate context-aware answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650b3102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Question-Answering chain created successfully\n"
     ]
    }
   ],
   "source": [
    "# Creating retriever from vector database (top 3 relevant chunks)\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Building the complete RetrievalQA chain with better configuration\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # Explicitly specifying chain type\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True, \n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": None  # Use default prompt which works better with FLAN-T5\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"RAG Question-Answering chain created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f570a3b9",
   "metadata": {},
   "source": [
    "## Step 8: Testing the RAG System\n",
    "\n",
    "We are testing our RAG-based question answering system with the sample questions provided in the instructions. This demonstrates how the system retrieves relevant TED talk content and generates context-aware answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b648ad58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: What do TED speakers say about climate change?\n",
      "\n",
      "Answer:\n",
      "a solvable problem that we can tackle together\n",
      "\n",
      "Source Documents:\n",
      "\n",
      "--- Source 1 ---\n",
      "\"Dare mighty things.\" These words are at the entrance to NASA's Jet Propulsion Laboratory (JPL), and JPL science communicator Laura Tenenbaum says they embody the attitude we must take towards climate...\n",
      "\n",
      "--- Source 2 ---\n",
      "A brief answer to one of the key questions about climate change: Why act now? (Written by Myles Allen, David Biello and George Zaidan)\n",
      "\n",
      "--- Source 3 ---\n",
      "Lighting up the TED stage, Nobel laureate Al Gore takes stock of the current state of climate progress and calls attention to institutions that have failed to honor their promises by continuing to pou...\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Question 2: What is the general opinion on education?\n",
      "\n",
      "Answer:\n",
      "Educator Nora Flanagan says we can reframe this moment as an opportunity to fix what's long been broken for teachers, students and families -- and shares four ways schools can reinvent themselves for a post-pandemic world\n",
      "\n",
      "Source Documents:\n",
      "\n",
      "--- Source 1 ---\n",
      "The abrupt shift to online learning due to COVID-19 rocked the US education system, unearthing many of the inequities at its foundation. Educator Nora Flanagan says we can reframe this moment as an op...\n",
      "\n",
      "--- Source 2 ---\n",
      "Less than seven percent of people worldwide have a bachelor's degree -- and for many, this is simply because the cost of university is too high, says higher education executive Adrian K. Haugabrook. I...\n",
      "\n",
      "--- Source 3 ---\n",
      "The world is changing rapidly but models of decades-old schooling still influence educational systems in ways that leave students with few reasons to truly excel. Education researcher Caitlin Holman e...\n"
     ]
    }
   ],
   "source": [
    "# A Q&A function with source document display\n",
    "def simple_qa_with_sources(question, retriever, pipeline):\n",
    "    # Get relevant documents\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    \n",
    "    # Combine all context\n",
    "    context = \" \".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    # Simple prompt format that works well with FLAN-T5\n",
    "    prompt = f\"Question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "    \n",
    "    # Generate answer\n",
    "    answer = pipeline(prompt, max_length=200, do_sample=True, temperature=0.7)[0]['generated_text']\n",
    "    \n",
    "    return {\n",
    "        \"result\": answer,\n",
    "        \"source_documents\": docs\n",
    "    }\n",
    "\n",
    "result1 = simple_qa_with_sources(\"What do TED speakers say about climate change?\", retriever, qa_pipeline)\n",
    "print(\"Question 1: What do TED speakers say about climate change?\")\n",
    "print(\"\\nAnswer:\")\n",
    "print(result1[\"result\"])\n",
    "print(\"\\nSource Documents:\")\n",
    "for i, doc in enumerate(result1[\"source_documents\"], 1):\n",
    "    print(f\"\\n--- Source {i} ---\")\n",
    "    print(doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "result2 = simple_qa_with_sources(\"What is the general opinion on education?\", retriever, qa_pipeline)\n",
    "print(\"Question 2: What is the general opinion on education?\")\n",
    "print(\"\\nAnswer:\")\n",
    "print(result2[\"result\"])\n",
    "print(\"\\nSource Documents:\")\n",
    "for i, doc in enumerate(result2[\"source_documents\"], 1):\n",
    "    print(f\"\\n--- Source {i} ---\")\n",
    "    print(doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3024231a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have successfully implemented a RAG-based question answering system using TED Talks data. The system demonstrates how retrieval-augmented generation improves language model responses by grounding them in real data. \n",
    "\n",
    "### Key Components Implemented:\n",
    "- **Data Loading**: TED Talks dataset with 1000 samples\n",
    "- **Document Processing**: Conversion to LangChain documents with metadata\n",
    "- **Text Chunking**: Optimal splitting for retrieval performance\n",
    "- **Embeddings**: Semantic vector representations using sentence transformers\n",
    "- **Vector Database**: FAISS for efficient similarity search\n",
    "- **Language Model**: FLAN-T5 for context-aware answer generation\n",
    "- **RAG Chain**: Complete integration of retrieval and generation components\n",
    "\n",
    "### System Capabilities:\n",
    "The RAG system can answer questions about various topics covered in TED talks by retrieving relevant content and generating informed responses based on the retrieved context.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
