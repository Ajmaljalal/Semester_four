{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5718aeb9",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with BERT on IMDb Movie Reviews\n",
    "\n",
    "In this assignment, we are implementing a sentiment analysis model using BERT (Bidirectional Encoder Representations from Transformers) to classify IMDb movie reviews as positive or negative. We will be using the pre-trained BERT model from the Transformers library and fine-tuning it on the IMDb dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33416e1",
   "metadata": {},
   "source": [
    "## Setting up Environment\n",
    "\n",
    "We are importing the necessary libraries for our sentiment analysis task including transformers for BERT, PyTorch for deep learning, pandas for data handling, and scikit-learn for evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92efa675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT components imported successfully!\n",
      "Environment setup completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Installing and importing required libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import transformers components (AdamW is now in torch.optim)\n",
    "try:\n",
    "    from transformers import BertTokenizer, BertForSequenceClassification\n",
    "    print(\"BERT components imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"Import error: {e}\")\n",
    "\n",
    "# Import optimizer from torch (AdamW moved from transformers to torch.optim)\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Setting random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Environment setup completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1349c4",
   "metadata": {},
   "source": [
    "## Loading and Exploring the Dataset\n",
    "\n",
    "We are loading the IMDB movie review dataset which contains 50,000 movie reviews labeled as positive or negative. We will examine the structure and basic statistics of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5eb2078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (50000, 2)\n",
      "Columns: ['review', 'sentiment']\n",
      "\n",
      "First few rows:\n",
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "# Loading the IMDB dataset\n",
    "df = pd.read_csv('IMDB-Dataset.csv')\n",
    "\n",
    "# Displaying basic information about the dataset\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70783ff",
   "metadata": {},
   "source": [
    "## Data Exploration and Preprocessing\n",
    "\n",
    "We are exploring the dataset distribution and preprocessing the text data by cleaning HTML tags and preparing sentiment labels for binary classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ded4b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment distribution:\n",
      "sentiment\n",
      "positive    25000\n",
      "negative    25000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataset balance: sentiment\n",
      "positive    0.5\n",
      "negative    0.5\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Missing values:\n",
      "review       0\n",
      "sentiment    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Exploring dataset distribution\n",
    "print(\"Sentiment distribution:\")\n",
    "print(df['sentiment'].value_counts())\n",
    "print(f\"\\nDataset balance: {df['sentiment'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Checking for missing values\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3851bd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text preprocessing completed!\n",
      "Sample cleaned review: A wonderful little production. The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. The actors...\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing text data - removing HTML tags and cleaning\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "# Applying text cleaning\n",
    "df['cleaned_review'] = df['review'].apply(clean_text)\n",
    "\n",
    "# Converting sentiment labels to numerical format (0: negative, 1: positive)\n",
    "df['label'] = df['sentiment'].map({'negative': 0, 'positive': 1})\n",
    "\n",
    "print(\"Text preprocessing completed!\")\n",
    "print(f\"Sample cleaned review: {df['cleaned_review'].iloc[1][:200]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a18a26",
   "metadata": {},
   "source": [
    "## Splitting Dataset into Training and Testing Sets\n",
    "\n",
    "We are splitting the dataset into training and testing sets to evaluate our model's performance on unseen data. We will use 80% for training and 20% for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5402df7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 40000\n",
      "Testing set size: 10000\n",
      "Training label distribution: label\n",
      "0    20000\n",
      "1    20000\n",
      "Name: count, dtype: int64\n",
      "Testing label distribution: label\n",
      "0    5000\n",
      "1    5000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['cleaned_review'], \n",
    "    df['label'], \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=df['label']\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")\n",
    "print(f\"Training label distribution: {y_train.value_counts().sort_index()}\")\n",
    "print(f\"Testing label distribution: {y_test.value_counts().sort_index()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c6d7fc",
   "metadata": {},
   "source": [
    "## Setting up BERT Tokenizer\n",
    "\n",
    "We are initializing the BERT tokenizer which will convert our text reviews into tokens that BERT can understand. We will use the 'bert-base-uncased' model for tokenization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40d2ee45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT tokenizer initialized successfully!\n",
      "Vocabulary size: 30522\n",
      "Maximum sequence length: 256\n"
     ]
    }
   ],
   "source": [
    "# Initializing BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Setting maximum sequence length for BERT (512 is the maximum for BERT)\n",
    "MAX_LEN = 256  # Using 256 for faster training while maintaining good performance\n",
    "\n",
    "print(\"BERT tokenizer initialized successfully!\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Maximum sequence length: {MAX_LEN}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6307262b",
   "metadata": {},
   "source": [
    "## Creating BERT Input Features\n",
    "\n",
    "We are converting text reviews into BERT input features including input_ids, attention_masks, and token_type_ids. This process tokenizes the text and prepares it for BERT model input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd597477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding training data...\n",
      "Encoding testing data...\n",
      "Text encoding completed!\n",
      "Training input shape: torch.Size([40000, 256])\n",
      "Testing input shape: torch.Size([10000, 256])\n"
     ]
    }
   ],
   "source": [
    "# Function to encode text using BERT tokenizer\n",
    "def encode_texts(texts, tokenizer, max_len):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for text in texts:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    \n",
    "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n",
    "\n",
    "# Encoding training and testing data\n",
    "print(\"Encoding training data...\")\n",
    "train_input_ids, train_attention_masks = encode_texts(X_train.tolist(), tokenizer, MAX_LEN)\n",
    "\n",
    "print(\"Encoding testing data...\")\n",
    "test_input_ids, test_attention_masks = encode_texts(X_test.tolist(), tokenizer, MAX_LEN)\n",
    "\n",
    "print(\"Text encoding completed!\")\n",
    "print(f\"Training input shape: {train_input_ids.shape}\")\n",
    "print(f\"Testing input shape: {test_input_ids.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecadb272",
   "metadata": {},
   "source": [
    "## Creating PyTorch Dataset and DataLoader\n",
    "\n",
    "We are creating a custom PyTorch Dataset class to handle our BERT inputs and creating DataLoaders for efficient batch processing during training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69d0d3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset objects created successfully!\n",
      "Training dataset size: 40000\n",
      "Testing dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "# Custom Dataset class for BERT inputs\n",
    "class IMDbDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_masks[idx],\n",
    "            'labels': torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Converting labels to tensors\n",
    "train_labels = torch.tensor(y_train.values, dtype=torch.long)\n",
    "test_labels = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Creating dataset objects\n",
    "train_dataset = IMDbDataset(train_input_ids, train_attention_masks, y_train)\n",
    "test_dataset = IMDbDataset(test_input_ids, test_attention_masks, y_test)\n",
    "\n",
    "print(\"Dataset objects created successfully!\")\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Testing dataset size: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca6b30bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders created successfully!\n",
      "Training batches: 2500\n",
      "Testing batches: 625\n",
      "Batch size: 16\n"
     ]
    }
   ],
   "source": [
    "# Creating DataLoaders for batch processing\n",
    "BATCH_SIZE = 16  # Using smaller batch size for memory efficiency\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0  # Set to 0 for compatibility\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(\"DataLoaders created successfully!\")\n",
    "print(f\"Training batches: {len(train_dataloader)}\")\n",
    "print(f\"Testing batches: {len(test_dataloader)}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76586256",
   "metadata": {},
   "source": [
    "## Loading Pre-trained BERT Model\n",
    "\n",
    "We are loading the pre-trained BERT model for sequence classification. The model will be fine-tuned for binary sentiment classification with 2 output classes (negative and positive).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2248754e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model loaded successfully!\n",
      "Device: cpu\n",
      "Model parameters: 109,483,778\n"
     ]
    }
   ],
   "source": [
    "# Loading pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2,  # Binary classification (negative=0, positive=1)\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "\n",
    "# Checking if GPU is available and moving model to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"BERT model loaded successfully!\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9539f8d",
   "metadata": {},
   "source": [
    "## Setting up Training Configuration\n",
    "\n",
    "We are configuring the training parameters including optimizer, learning rate, and number of epochs for fine-tuning the BERT model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "057bdc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration completed!\n",
      "Epochs: 4\n",
      "Learning rate: 2e-05\n",
      "Optimizer: AdamW\n",
      "Total training steps: 10000\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 4  # Using 4 epochs for demonstration (should be increased for better performance)\n",
    "LEARNING_RATE = 2e-5  # Recommended learning rate for BERT fine-tuning\n",
    "\n",
    "# Setting up optimizer\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    eps=1e-8  # Default epsilon value for AdamW\n",
    ")\n",
    "\n",
    "# Setting up loss function (CrossEntropyLoss is used automatically by BERT model)\n",
    "print(\"Training configuration completed!\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Optimizer: AdamW\")\n",
    "print(f\"Total training steps: {len(train_dataloader) * EPOCHS}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011a6b15",
   "metadata": {},
   "source": [
    "## Fine-tuning BERT Model\n",
    "\n",
    "We are implementing the training loop to fine-tune the BERT model on our IMDb dataset. The model will learn to classify movie reviews as positive or negative sentiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f2fcba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BERT fine-tuning...\n"
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "def train_model(model, train_dataloader, optimizer, device, epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch in train_dataloader:\n",
    "            # Moving batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = outputs.loss\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_loss += epoch_loss\n",
    "    \n",
    "    return total_loss / (epochs * len(train_dataloader))\n",
    "\n",
    "# Fine-tuning the model\n",
    "print(\"Starting BERT fine-tuning...\")\n",
    "avg_loss = train_model(model, train_dataloader, optimizer, device, EPOCHS)\n",
    "print(f\"Training completed! Average loss: {avg_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
